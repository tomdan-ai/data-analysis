{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9ccfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Pandas version: 2.3.0\n",
      "NumPy version: 2.1.0\n",
      "Created directory: saved_datasets\n",
      "\n",
      "LOADING DATASET\n",
      "Available datasets: ['New_Social_Network_Ads_Data.csv', 'New_Social_Network_Ads_Data.xlsx', 'Social_Network_Ads_CSV.csv', 'NoIndex_Social_Network_Ads_Data.csv', 'Social_Network_Ads.csv', 'Social_Network_Ads_JSON.json', 'Social_Network_Ads_Excel.xlsx', 'New_Social_Network_Ads_Data.json', 'Social_Network_Ads_Delimiter.csv']\n",
      "Creating sample Social Network Ads dataset for demonstration...\n",
      "Sample dataset created successfully!\n",
      "Dataset shape: (405, 5)\n",
      "Columns: ['User ID', 'Gender', 'Age', 'EstimatedSalary', 'Purchased']\n",
      "\n",
      "Dataset Overview:\n",
      "    User ID  Gender  Age  EstimatedSalary  Purchased\n",
      "0  15624807    Male   58         128746.0          1\n",
      "1  15624808  Female   47          53889.0          1\n",
      "2  15624809  Female   34          63669.0          0\n",
      "3  15624810  Female   37          72988.0          1\n",
      "4  15624811    Male   42          80287.0          1\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 405 entries, 0 to 404\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   User ID          405 non-null    int64  \n",
      " 1   Gender           405 non-null    object \n",
      " 2   Age              405 non-null    int64  \n",
      " 3   EstimatedSalary  395 non-null    float64\n",
      " 4   Purchased        405 non-null    int64  \n",
      "dtypes: float64(1), int64(3), object(1)\n",
      "memory usage: 15.9+ KB\n",
      "\n",
      "BASIC DATA SAVING OPERATIONS\n",
      "\n",
      "STEP 1: Save DataFrame to CSV file\n",
      "Saved to CSV: saved_datasets/New_Social_Network_Ads_Data.csv\n",
      "File size: 11350 bytes\n",
      "Verification - Shape after reading: (405, 5)\n",
      "First 3 rows:\n",
      "    User ID  Gender  Age  EstimatedSalary  Purchased\n",
      "0  15624807    Male   58         128746.0          1\n",
      "1  15624808  Female   47          53889.0          1\n",
      "2  15624809  Female   34          63669.0          0\n",
      "\n",
      "\n",
      "STEP 2: Save DataFrame to Excel\n",
      "Saved to Excel: saved_datasets/New_Social_Network_Ads_Data.xlsx\n",
      "File size: 15861 bytes\n",
      "Verification - Shape after reading: (405, 5)\n",
      "\n",
      "\n",
      "STEP 3: Write to multiple Excel sheets\n",
      "Saved multi-sheet Excel: saved_datasets/Multiple_Social_Network_Ads_Data.xlsx\n",
      "Sheets created: AllData, MaleUsers, FemaleUsers, Summary\n",
      "Sheet names: ['AllData', 'MaleUsers', 'FemaleUsers', 'Summary']\n",
      "\n",
      "\n",
      "STEP 4: Save DataFrame to JSON\n",
      "Saved JSON (records, lines=True): saved_datasets/Social_Network_Ads_Data_records.json\n",
      "File size: 35240 bytes\n",
      "Saved JSON (index): saved_datasets/Social_Network_Ads_Data_index.json\n",
      "File size: 50117 bytes\n",
      "Saved JSON (values): saved_datasets/Social_Network_Ads_Data_values.json\n",
      "File size: 25522 bytes\n",
      "Saved JSON (columns): saved_datasets/Social_Network_Ads_Data_columns.json\n",
      "File size: 33977 bytes\n",
      "\n",
      "Reading JSON files back:\n",
      "records: Shape (405, 5)\n",
      "index: Shape (405, 5)\n",
      "values: Shape (405, 5)\n",
      "columns: Shape (405, 5)\n",
      "\n",
      "\n",
      "STEP 5: Save DataFrame to Parquet (Optional)\n",
      "Parquet export requires pyarrow or fastparquet. Install with:\n",
      "pip install pyarrow\n",
      "or\n",
      "pip install fastparquet\n",
      "\n",
      "INDEX AND PARAMETER CONTROL\n",
      "\n",
      "STEP 6: Save CSV with different index settings\n",
      "Saved WITH index: saved_datasets/Indexed_Social_Network_Ads_Data.csv\n",
      "Saved WITHOUT index: saved_datasets/NoIndex_Social_Network_Ads_Data.csv\n",
      "With index file size: 12861 bytes\n",
      "Without index file size: 11350 bytes\n",
      "Difference: 1511 bytes\n",
      "\n",
      "Content comparison (first 3 lines):\n",
      "WITH index:\n",
      ",User ID,Gender,Age,EstimatedSalary,Purchased\n",
      "0,15624807,Male,58,128746.0,1\n",
      "1,15624808,Female,47,53889.0,1\n",
      "WITHOUT index:\n",
      "User ID,Gender,Age,EstimatedSalary,Purchased\n",
      "15624807,Male,58,128746.0,1\n",
      "15624808,Female,47,53889.0,1\n",
      "\n",
      "DATA PROCESSING PIPELINE WITH INTERMEDIATE SAVES\n",
      "\n",
      "STEP 7: Data processing pipeline with intermediate saves\n",
      "Original dataset shape: (405, 5)\n",
      "Original duplicates: 5\n",
      "Original missing values: 10\n",
      "\n",
      "1. Removing duplicates...\n",
      "Removed 5 duplicates\n",
      "Saved cleaned data: saved_datasets/Cleaned_Social_Network_Ads_Data.csv\n",
      "New shape: (400, 5)\n",
      "\n",
      "2. Handling missing values...\n",
      "Removed 10 rows with missing values\n",
      "Saved data without missing values: saved_datasets/NoMissing_Social_Network_Ads_Data.csv\n",
      "New shape: (390, 5)\n",
      "\n",
      "3. Filtering data (Age > 40)...\n",
      "Filtered to 167 records (Age > 40)\n",
      "Saved filtered data: saved_datasets/Filtered_Social_Network_Ads_Data.xlsx\n",
      "Final shape: (167, 5)\n",
      "\n",
      "4. Creating age groups analysis...\n",
      "Created age group analysis: saved_datasets/Age_Group_Analysis.csv\n",
      "Age Group Summary:\n",
      "          User_Count  Avg_Salary  Purchase_Rate\n",
      "AgeGroup                                       \n",
      "<30                0         NaN            NaN\n",
      "30-40              0         NaN            NaN\n",
      "40-50             85    71001.12           0.40\n",
      ">50               79    73421.75           0.37\n",
      "\n",
      "ADVANCED SAVING OPTIONS\n",
      "\n",
      "Advanced CSV saving options:\n",
      "Saved with custom separator (|): saved_datasets/Custom_Separator_Data.txt\n",
      "Saved selected columns only: saved_datasets/Selected_Columns_Data.csv\n",
      "Saved with custom headers: saved_datasets/Custom_Headers_Data.csv\n",
      "Demonstrated append mode: saved_datasets/Append_Demo_Data.csv\n",
      "\n",
      "FILE SUMMARY AND VERIFICATION\n",
      "\n",
      "All created files:\n",
      "Age_Group_Analysis.csv                   0.1 KB\n",
      "Append_Demo_Data.csv                     0.4 KB\n",
      "Cleaned_Social_Network_Ads_Data.csv     10.9 KB\n",
      "Custom_Headers_Data.csv                  4.6 KB\n",
      "Custom_Separator_Data.txt                5.5 KB\n",
      "Filtered_Social_Network_Ads_Data.xlsx      9.2 KB\n",
      "Indexed_Social_Network_Ads_Data.csv     12.6 KB\n",
      "Multiple_Social_Network_Ads_Data.xlsx     27.5 KB\n",
      "New_Social_Network_Ads_Data.csv         11.1 KB\n",
      "New_Social_Network_Ads_Data.xlsx        15.5 KB\n",
      "NoIndex_Social_Network_Ads_Data.csv     11.1 KB\n",
      "NoMissing_Social_Network_Ads_Data.csv     10.7 KB\n",
      "Selected_Columns_Data.csv                2.3 KB\n",
      "Social_Network_Ads_Data_columns.json     33.2 KB\n",
      "Social_Network_Ads_Data_index.json      48.9 KB\n",
      "Social_Network_Ads_Data_records.json     34.4 KB\n",
      "Social_Network_Ads_Data_values.json     24.9 KB\n",
      "Total files created: 17\n",
      "Total disk space used: 263.0 KB\n",
      "\n",
      "READING VERIFICATION\n",
      "\n",
      "Reading back saved files to verify integrity:\n",
      "CSV      - Shape: (405, 5), Columns: 5\n",
      "JSON     - Shape: (405, 5), Columns: 5\n",
      "Excel    - Shape: (405, 5), Columns: 5\n",
      "\n",
      "LAB 2.4 COMPLETED SUCCESSFULLY!\n",
      "\n",
      "Summary of Operations Completed:\n",
      "   CSV file saving (with/without index)\n",
      "   Excel file saving (single and multi-sheet)\n",
      "   JSON file saving (multiple orientations)\n",
      "   Parquet file saving (if available)\n",
      "   Data processing pipeline with intermediate saves\n",
      "   Advanced saving options and parameters\n",
      "   File verification and integrity checks\n",
      "\n",
      "All files saved in: saved_datasets/\n",
      "Dataset processed: Social Network Ads Data\n",
      "Pipeline: Raw -> Cleaned -> Filtered -> Analyzed\n",
      "\n",
      "Next steps: Explore the saved files and experiment with different parameters!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LAB 2.4: Basic Data Saving\n",
    "# Aim: Learn how to export cleaned or transformed datasets to various file formats.\n",
    "\n",
    "# Objectives:\n",
    "# Save a DataFrame to a .csv file using DataFrame.to_csv().\n",
    "# Write Excel files using DataFrame.to_excel(), including to specific sheets.\n",
    "# Export structured data to JSON using DataFrame.to_json() with appropriate orientation.\n",
    "# Save to and read from Parquet files using DataFrame.to_parquet() (optional: install pyarrow or fastparquet).\n",
    "# Use if_exists and index parameters to control overwriting and metadata saving.\n",
    "# Practice saving intermediate processing outputs during a data analysis pipeline.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "# Create output directory for saved files\n",
    "output_dir = \"saved_datasets\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created directory: {output_dir}\")\n",
    "\n",
    "print(\"\\nLOADING DATASET\")\n",
    "\n",
    "# Try to load dataset from datasets folder, otherwise create sample data\n",
    "try:\n",
    "    # Try to load from datasets folder - you can specify your actual dataset here\n",
    "    dataset_files = []\n",
    "    if os.path.exists('datasets'):\n",
    "        dataset_files = [f for f in os.listdir('datasets') if f.endswith(('.csv', '.xlsx', '.json'))]\n",
    "        print(f\"Available datasets: {dataset_files}\")\n",
    "    \n",
    "    # If you have a specific dataset, load it here:\n",
    "    # df = pd.read_csv('datasets/your_dataset.csv')\n",
    "    \n",
    "    # For demonstration, create sample Social Network Ads data similar to the lab\n",
    "    print(\"Creating sample Social Network Ads dataset for demonstration...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 400\n",
    "    \n",
    "    # Create realistic Social Network Ads data\n",
    "    df = pd.DataFrame({\n",
    "        'User ID': range(15624807, 15624807 + n_samples),\n",
    "        'Gender': np.random.choice(['Male', 'Female'], n_samples, p=[0.52, 0.48]),\n",
    "        'Age': np.random.randint(18, 60, n_samples),\n",
    "        'EstimatedSalary': np.random.normal(69000, 25000, n_samples).astype(int),\n",
    "        'Purchased': np.random.choice([0, 1], n_samples, p=[0.64, 0.36])\n",
    "    })\n",
    "    \n",
    "    # Add some data quality issues for cleaning demonstration\n",
    "    # Add duplicates\n",
    "    df = pd.concat([df, df.iloc[:5]], ignore_index=True)\n",
    "    \n",
    "    # Add some missing values\n",
    "    df.loc[np.random.choice(df.index, 10, replace=False), 'EstimatedSalary'] = np.nan\n",
    "    \n",
    "    # Add some outliers\n",
    "    df.loc[np.random.choice(df.index, 3, replace=False), 'Age'] = np.random.choice([100, 150, 200])\n",
    "    \n",
    "    print(\"Sample dataset created successfully!\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Please check your datasets folder or modify the loading code above.\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nBASIC DATA SAVING OPERATIONS\")\n",
    "\n",
    "# STEP 1: Save a DataFrame to a CSV file\n",
    "print(\"\\nSTEP 1: Save DataFrame to CSV file\")\n",
    "\n",
    "csv_filename = os.path.join(output_dir, \"New_Social_Network_Ads_Data.csv\")\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"Saved to CSV: {csv_filename}\")\n",
    "\n",
    "# Verify the saved file\n",
    "if os.path.exists(csv_filename):\n",
    "    file_size = os.path.getsize(csv_filename)\n",
    "    print(f\"File size: {file_size} bytes\")\n",
    "    \n",
    "    # Read back and display first few rows\n",
    "    df_csv_check = pd.read_csv(csv_filename)\n",
    "    print(f\"Verification - Shape after reading: {df_csv_check.shape}\")\n",
    "    print(f\"First 3 rows:\")\n",
    "    print(df_csv_check.head(3))\n",
    "\n",
    "# STEP 2: Save a DataFrame to Excel\n",
    "print(\"\\n\\nSTEP 2: Save DataFrame to Excel\")\n",
    "\n",
    "try:\n",
    "    excel_filename = os.path.join(output_dir, \"New_Social_Network_Ads_Data.xlsx\")\n",
    "    df.to_excel(excel_filename, sheet_name=\"AdsData\", index=False)\n",
    "    print(f\"Saved to Excel: {excel_filename}\")\n",
    "    \n",
    "    # Verify the saved file\n",
    "    if os.path.exists(excel_filename):\n",
    "        file_size = os.path.getsize(excel_filename)\n",
    "        print(f\"File size: {file_size} bytes\")\n",
    "        \n",
    "        # Read back and verify\n",
    "        df_excel_check = pd.read_excel(excel_filename, sheet_name=\"AdsData\")\n",
    "        print(f\"Verification - Shape after reading: {df_excel_check.shape}\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"Excel export requires openpyxl or xlsxwriter. Install with:\")\n",
    "    print(\"pip install openpyxl\")\n",
    "    print(\"or\")\n",
    "    print(\"pip install xlsxwriter\")\n",
    "\n",
    "# STEP 3: Write to multiple Excel sheets\n",
    "print(\"\\n\\nSTEP 3: Write to multiple Excel sheets\")\n",
    "\n",
    "try:\n",
    "    multi_excel_filename = os.path.join(output_dir, \"Multiple_Social_Network_Ads_Data.xlsx\")\n",
    "    \n",
    "    # Create different views of the data for different sheets\n",
    "    df_male = df[df['Gender'] == 'Male'].copy()\n",
    "    df_female = df[df['Gender'] == 'Female'].copy()\n",
    "    \n",
    "    with pd.ExcelWriter(multi_excel_filename, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name=\"AllData\", index=False)\n",
    "        df_male.to_excel(writer, sheet_name=\"MaleUsers\", index=False)\n",
    "        df_female.to_excel(writer, sheet_name=\"FemaleUsers\", index=False)\n",
    "        \n",
    "        # Create a summary sheet\n",
    "        summary_data = {\n",
    "            'Metric': ['Total Users', 'Male Users', 'Female Users', 'Average Age', 'Average Salary', 'Purchase Rate'],\n",
    "            'Value': [\n",
    "                len(df),\n",
    "                len(df_male),\n",
    "                len(df_female),\n",
    "                f\"{df['Age'].mean():.1f}\",\n",
    "                f\"${df['EstimatedSalary'].mean():.0f}\",\n",
    "                f\"{df['Purchased'].mean():.2%}\"\n",
    "            ]\n",
    "        }\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "    \n",
    "    print(f\"Saved multi-sheet Excel: {multi_excel_filename}\")\n",
    "    print(f\"Sheets created: AllData, MaleUsers, FemaleUsers, Summary\")\n",
    "    \n",
    "    # Verify sheets\n",
    "    xl_file = pd.ExcelFile(multi_excel_filename)\n",
    "    print(f\"Sheet names: {xl_file.sheet_names}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Excel export requires openpyxl. Install with: pip install openpyxl\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating multi-sheet Excel: {e}\")\n",
    "\n",
    "# STEP 4: Save a DataFrame to JSON\n",
    "print(\"\\n\\nSTEP 4: Save DataFrame to JSON\")\n",
    "\n",
    "# Different JSON orientations\n",
    "json_orientations = ['records', 'index', 'values', 'columns']\n",
    "\n",
    "for orient in json_orientations:\n",
    "    json_filename = os.path.join(output_dir, f\"Social_Network_Ads_Data_{orient}.json\")\n",
    "    \n",
    "    if orient == 'records':\n",
    "        # For records orientation, also demonstrate lines=True\n",
    "        df.to_json(json_filename, orient=orient, lines=True)\n",
    "        print(f\"Saved JSON ({orient}, lines=True): {json_filename}\")\n",
    "    else:\n",
    "        df.to_json(json_filename, orient=orient, indent=2)\n",
    "        print(f\"Saved JSON ({orient}): {json_filename}\")\n",
    "    \n",
    "    # Show file size\n",
    "    if os.path.exists(json_filename):\n",
    "        file_size = os.path.getsize(json_filename)\n",
    "        print(f\"File size: {file_size} bytes\")\n",
    "\n",
    "# Demonstrate reading JSON back\n",
    "print(\"\\nReading JSON files back:\")\n",
    "for orient in json_orientations:\n",
    "    json_filename = os.path.join(output_dir, f\"Social_Network_Ads_Data_{orient}.json\")\n",
    "    try:\n",
    "        if orient == 'records':\n",
    "            df_json_check = pd.read_json(json_filename, lines=True)\n",
    "        else:\n",
    "            df_json_check = pd.read_json(json_filename, orient=orient)\n",
    "        print(f\"{orient}: Shape {df_json_check.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{orient}: Error - {e}\")\n",
    "\n",
    "# STEP 5: Save to Parquet (if available)\n",
    "print(\"\\n\\nSTEP 5: Save DataFrame to Parquet (Optional)\")\n",
    "\n",
    "try:\n",
    "    parquet_filename = os.path.join(output_dir, \"Social_Network_Ads_Data.parquet\")\n",
    "    df.to_parquet(parquet_filename, index=False)\n",
    "    print(f\"Saved to Parquet: {parquet_filename}\")\n",
    "    \n",
    "    # Verify file\n",
    "    if os.path.exists(parquet_filename):\n",
    "        file_size = os.path.getsize(parquet_filename)\n",
    "        print(f\"File size: {file_size} bytes\")\n",
    "        \n",
    "        # Read back and verify\n",
    "        df_parquet_check = pd.read_parquet(parquet_filename)\n",
    "        print(f\"Verification - Shape after reading: {df_parquet_check.shape}\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"Parquet export requires pyarrow or fastparquet. Install with:\")\n",
    "    print(\"pip install pyarrow\")\n",
    "    print(\"or\")\n",
    "    print(\"pip install fastparquet\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving Parquet: {e}\")\n",
    "\n",
    "print(\"\\nINDEX AND PARAMETER CONTROL\")\n",
    "\n",
    "# STEP 6: Save CSV with and without index\n",
    "print(\"\\nSTEP 6: Save CSV with different index settings\")\n",
    "\n",
    "# With index\n",
    "indexed_filename = os.path.join(output_dir, \"Indexed_Social_Network_Ads_Data.csv\")\n",
    "df.to_csv(indexed_filename, index=True)\n",
    "print(f\"Saved WITH index: {indexed_filename}\")\n",
    "\n",
    "# Without index\n",
    "no_index_filename = os.path.join(output_dir, \"NoIndex_Social_Network_Ads_Data.csv\")\n",
    "df.to_csv(no_index_filename, index=False)\n",
    "print(f\"Saved WITHOUT index: {no_index_filename}\")\n",
    "\n",
    "# Compare file sizes\n",
    "indexed_size = os.path.getsize(indexed_filename)\n",
    "no_index_size = os.path.getsize(no_index_filename)\n",
    "print(f\"With index file size: {indexed_size} bytes\")\n",
    "print(f\"Without index file size: {no_index_size} bytes\")\n",
    "print(f\"Difference: {indexed_size - no_index_size} bytes\")\n",
    "\n",
    "# Show the difference in content\n",
    "print(\"\\nContent comparison (first 3 lines):\")\n",
    "print(\"WITH index:\")\n",
    "with open(indexed_filename, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 3:\n",
    "            print(f\"{line.strip()}\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "print(\"WITHOUT index:\")\n",
    "with open(no_index_filename, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i < 3:\n",
    "            print(f\"{line.strip()}\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "print(\"\\nDATA PROCESSING PIPELINE WITH INTERMEDIATE SAVES\")\n",
    "\n",
    "# STEP 7: Saving Intermediate Outputs\n",
    "print(\"\\nSTEP 7: Data processing pipeline with intermediate saves\")\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Original duplicates: {df.duplicated().sum()}\")\n",
    "print(f\"Original missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Step 1: Remove duplicates\n",
    "print(\"\\n1. Removing duplicates...\")\n",
    "df_cleaned = df.drop_duplicates()\n",
    "cleaned_filename = os.path.join(output_dir, \"Cleaned_Social_Network_Ads_Data.csv\")\n",
    "df_cleaned.to_csv(cleaned_filename, index=False)\n",
    "print(f\"Removed {len(df) - len(df_cleaned)} duplicates\")\n",
    "print(f\"Saved cleaned data: {cleaned_filename}\")\n",
    "print(f\"New shape: {df_cleaned.shape}\")\n",
    "\n",
    "# Step 2: Handle missing values\n",
    "print(\"\\n2. Handling missing values...\")\n",
    "df_no_missing = df_cleaned.dropna()\n",
    "no_missing_filename = os.path.join(output_dir, \"NoMissing_Social_Network_Ads_Data.csv\")\n",
    "df_no_missing.to_csv(no_missing_filename, index=False)\n",
    "print(f\"Removed {len(df_cleaned) - len(df_no_missing)} rows with missing values\")\n",
    "print(f\"Saved data without missing values: {no_missing_filename}\")\n",
    "print(f\"New shape: {df_no_missing.shape}\")\n",
    "\n",
    "# Step 3: Filter data (Age > 40)\n",
    "print(\"\\n3. Filtering data (Age > 40)...\")\n",
    "df_transformed = df_no_missing[df_no_missing['Age'] > 40].copy()\n",
    "filtered_filename = os.path.join(output_dir, \"Filtered_Social_Network_Ads_Data.xlsx\")\n",
    "\n",
    "try:\n",
    "    df_transformed.to_excel(filtered_filename, index=False)\n",
    "    print(f\"Filtered to {len(df_transformed)} records (Age > 40)\")\n",
    "    print(f\"Saved filtered data: {filtered_filename}\")\n",
    "    print(f\"Final shape: {df_transformed.shape}\")\n",
    "except ImportError:\n",
    "    # If Excel is not available, save as CSV\n",
    "    filtered_csv_filename = os.path.join(output_dir, \"Filtered_Social_Network_Ads_Data.csv\")\n",
    "    df_transformed.to_csv(filtered_csv_filename, index=False)\n",
    "    print(f\"Filtered to {len(df_transformed)} records (Age > 40)\")\n",
    "    print(f\"Saved filtered data (CSV): {filtered_csv_filename}\")\n",
    "    print(f\"Final shape: {df_transformed.shape}\")\n",
    "\n",
    "# Step 4: Create age groups and save\n",
    "print(\"\\n4. Creating age groups analysis...\")\n",
    "df_transformed['AgeGroup'] = pd.cut(df_transformed['Age'], \n",
    "                                   bins=[0, 30, 40, 50, 100], \n",
    "                                   labels=['<30', '30-40', '40-50', '>50'])\n",
    "\n",
    "age_group_summary = df_transformed.groupby('AgeGroup').agg({\n",
    "    'User ID': 'count',\n",
    "    'EstimatedSalary': 'mean',\n",
    "    'Purchased': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "age_group_summary.columns = ['User_Count', 'Avg_Salary', 'Purchase_Rate']\n",
    "age_summary_filename = os.path.join(output_dir, \"Age_Group_Analysis.csv\")\n",
    "age_group_summary.to_csv(age_summary_filename)\n",
    "print(f\"Created age group analysis: {age_summary_filename}\")\n",
    "print(\"Age Group Summary:\")\n",
    "print(age_group_summary)\n",
    "\n",
    "print(\"\\nADVANCED SAVING OPTIONS\")\n",
    "\n",
    "# Demonstrate various CSV parameters\n",
    "print(\"\\nAdvanced CSV saving options:\")\n",
    "\n",
    "# Custom separator\n",
    "custom_sep_filename = os.path.join(output_dir, \"Custom_Separator_Data.txt\")\n",
    "df_transformed.to_csv(custom_sep_filename, sep='|', index=False)\n",
    "print(f\"Saved with custom separator (|): {custom_sep_filename}\")\n",
    "\n",
    "# Only specific columns\n",
    "selected_cols_filename = os.path.join(output_dir, \"Selected_Columns_Data.csv\")\n",
    "df_transformed[['User ID', 'Age', 'Purchased']].to_csv(selected_cols_filename, index=False)\n",
    "print(f\"Saved selected columns only: {selected_cols_filename}\")\n",
    "\n",
    "# With custom headers\n",
    "custom_header_filename = os.path.join(output_dir, \"Custom_Headers_Data.csv\")\n",
    "df_transformed.to_csv(custom_header_filename, \n",
    "                     columns=['User ID', 'Gender', 'Age', 'EstimatedSalary', 'Purchased'],\n",
    "                     header=['ID', 'Sex', 'Years', 'Income', 'Bought'],\n",
    "                     index=False)\n",
    "print(f\"Saved with custom headers: {custom_header_filename}\")\n",
    "\n",
    "# Append mode demonstration (be careful with this)\n",
    "append_filename = os.path.join(output_dir, \"Append_Demo_Data.csv\")\n",
    "df_small = df_transformed.head(5)\n",
    "df_small.to_csv(append_filename, index=False, mode='w')  # Write first\n",
    "df_small.to_csv(append_filename, index=False, mode='a', header=False)  # Append without header\n",
    "print(f\"Demonstrated append mode: {append_filename}\")\n",
    "\n",
    "print(\"\\nFILE SUMMARY AND VERIFICATION\")\n",
    "\n",
    "# List all created files\n",
    "print(\"\\nAll created files:\")\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    files = os.listdir(output_dir)\n",
    "    total_size = 0\n",
    "    \n",
    "    for file in sorted(files):\n",
    "        file_path = os.path.join(output_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size = os.path.getsize(file_path)\n",
    "            size_kb = size / 1024\n",
    "            total_size += size\n",
    "            print(f\"{file:<35} {size_kb:>8.1f} KB\")\n",
    "    \n",
    "    print(f\"Total files created: {len(files)}\")\n",
    "    print(f\"Total disk space used: {total_size/1024:.1f} KB\")\n",
    "\n",
    "print(\"\\nREADING VERIFICATION\")\n",
    "\n",
    "# Verify we can read back all major formats\n",
    "print(\"\\nReading back saved files to verify integrity:\")\n",
    "\n",
    "test_files = [\n",
    "    (os.path.join(output_dir, \"New_Social_Network_Ads_Data.csv\"), \"CSV\", pd.read_csv),\n",
    "    (os.path.join(output_dir, \"Social_Network_Ads_Data_records.json\"), \"JSON\", lambda x: pd.read_json(x, lines=True)),\n",
    "]\n",
    "\n",
    "# Add Excel if available\n",
    "excel_file = os.path.join(output_dir, \"New_Social_Network_Ads_Data.xlsx\")\n",
    "if os.path.exists(excel_file):\n",
    "    test_files.append((excel_file, \"Excel\", pd.read_excel))\n",
    "\n",
    "# Add Parquet if available\n",
    "parquet_file = os.path.join(output_dir, \"Social_Network_Ads_Data.parquet\")\n",
    "if os.path.exists(parquet_file):\n",
    "    test_files.append((parquet_file, \"Parquet\", pd.read_parquet))\n",
    "\n",
    "for file_path, file_type, read_func in test_files:\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            df_test = read_func(file_path)\n",
    "            print(f\"{file_type:<8} - Shape: {df_test.shape}, Columns: {len(df_test.columns)}\")\n",
    "        else:\n",
    "            print(f\"{file_type:<8} - File not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"{file_type:<8} - Error: {str(e)[:50]}...\")\n",
    "\n",
    "print(\"\\nLAB 2.4 COMPLETED SUCCESSFULLY!\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Summary of Operations Completed:\n",
    "   CSV file saving (with/without index)\n",
    "   Excel file saving (single and multi-sheet)\n",
    "   JSON file saving (multiple orientations)\n",
    "   Parquet file saving (if available)\n",
    "   Data processing pipeline with intermediate saves\n",
    "   Advanced saving options and parameters\n",
    "   File verification and integrity checks\n",
    "\n",
    "All files saved in: {output_dir}/\n",
    "Dataset processed: Social Network Ads Data\n",
    "Pipeline: Raw -> Cleaned -> Filtered -> Analyzed\n",
    "\n",
    "Next steps: Explore the saved files and experiment with different parameters!\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
